<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/rehold.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model</h1>
          <h4 class="subtitle is-5"><em>CVPR 2025 </em> </h4>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yingying Fan<sup>1</sup>,</span>
            <span class="author-block">
              Quanwei Yang<sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=2Pedf3EAAAAJ">Kaisiyuan Wang</a><sup>3*</sup>,
            </span>
            <span class="author-block">
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Yingying Li<sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=pnuQ5UsAAAAJ&view_op=list_works&sortby=pubdate">Haocheng Feng</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://yu-wu.net/">Yu Wu</a><sup>1*</sup>
            </span>
            <span class="author-block">
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a><sup>3</sup>
            </span>
            
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Wuhan University,</span>
            <span class="author-block"><sup>2</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>3</sup>Department of Computer Vision Technology (VIS), Baidu Inc.</span>
            
          </div>

        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">

      <img src="./static/images/pipeline.png"  width="1000px" height="800px" 
        type="application/pdf">
      </img>
      
      <h2 class="subtitle has-text-centered">
        <b>The overview of our proposed framework Re-HOLD.</b>
      </h2>
    <p>We propose a two-branch framework that consists of a Reference U-Net and a Denoising U-Net. 
        The Reference U-Net takes a reference object image for object texture encoding while the denoising one takes noise latent and layout guidance as input for diffusion processing. 
        To enhance the quality of HOI generation, we adopt the HOI Restoration Module and a hand memory bank for hand information restoration. 
        The object memory bank is designed to store fine-grained object information.</p>

  </div>
</section>

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <source src="./static/videos/steve.mp4" type="video/mp4">
      </div>
    </div>
  </div>
</section>  -->
    





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Current digital human studies focusing on lip-syncing and body movement are no longer sufficient to meet the growing industrial demand, 
            while human video generation techniques that support interacting with real-world environments (e.g., objects) have not been well investigated.
            Despite human hand synthesis already being an intricate problem, generating objects in contact with hands and their interactions presents an even more challenging task, 
            especially when the objects exhibit obvious variations in size and shape. 
          </p>
          <p>
          To cope with these issues, we present a novel video Reenactment framework focusing on Human-Object Interaction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).
          Our key insight is to employ specialized layout representation for hands and objects, respectively. 
          Such representations enable effective disentanglement of hand modeling and object adaptation to diverse motion sequences.
          To further improve the generation quality of HOI, we have designed an interactive textural enhancement module for both hands and objects by introducing two independent memory banks.
          We also propose a layout-adjusting strategy for the cross-object reenactment scenario to adaptively adjust unreasonable layouts caused by diverse object sizes during inference.
          </p>
          <p>
            Comprehensive qualitative and quantitative evaluations demonstrate that our proposed framework significantly outperforms existing methods.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <!-- <div class="publication-video"> -->
          <!-- <video poster="" id="paper-video" autoplay controls muted loop playsinline height="100%"> -->
          <!-- <video width="800" height="600" controls>
            <source src="https://www.youtube.com/watch?v=jWvAglDGWxA&t=1s" type="video/mp4">
          </video> -->
          <iframe width="672" height="378" 
            src="https://www.youtube.com/embed/43n863HnvlQ?si=eaE-qtyo8Cv53Zis" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; 
            clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
          </iframe>
          <!-- </video> -->
        <!-- </div> -->
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<!-- <video width="320" height="240" controls>
  <source src="videos/movie.mp4" type="video/new_sup_final_crf20.mp4">
  Your browser does not support the video tag.
</video> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{fan2025ReHOLD,
  author    = {Yinying Fan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Yingying Li, Haocheng Feng, Yu Wu, Jingdong Wang.},
  title     = {Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model.},
  journal   = {CVPR},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            <!-- This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website. -->
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
